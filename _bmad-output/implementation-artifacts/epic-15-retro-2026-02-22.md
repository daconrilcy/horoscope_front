# Epic 15 Retrospective ‚Äî AI Text Engine (OpenAI Gateway)

**Date :** 2026-02-22  
**Participants :** √âquipe de d√©veloppement (Dev Agents + Cyril)  
**FRs couverts :** FR19, FR20, FR22 (am√©lioration couche LLM)

---

## R√©sum√© de l'Epic

L'Epic 15 a cr√©√© un moteur centralis√© de g√©n√©ration de texte IA, rempla√ßant l'ancien stub LLM par une int√©gration compl√®te avec OpenAI, incluant rate limiting, cache, observabilit√© et d√©ploiement Docker.

- **Story 15.1** : AI Text Engine ‚Äî Moteur de g√©n√©ration (OpenAI Gateway)
- **Story 15.2** : Rate limiting, observabilit√© et d√©ploiement Docker
- **Story 15.3** : Migration des services Chat et Guidance vers le AI Engine
- **Story 15.4** : Interpr√©tation textuelle du th√®me natal via AI Engine

---

## Ce qui a bien fonctionn√© ‚úÖ

### 1. Architecture modulaire et extensible
Le pattern Provider abstrait permet d'ajouter d'autres LLM (Anthropic, Mistral) sans modifier les services appelants :
```
Services ‚Üí AIEngineAdapter ‚Üí AI Engine ‚Üí ProviderClient ‚Üí OpenAI
```

### 2. Prompt Registry centralis√©
Tous les prompts sont version√©s dans des templates Jinja2 :
- `chat_system.jinja2`
- `natal_chart_interpretation_v1.jinja2`
- `guidance_daily_v1.jinja2`, `guidance_weekly_v1.jinja2`, `guidance_contextual_v1.jinja2`
- `card_reading_v1.jinja2`

### 3. Robustesse de production
- Rate limiting avec Redis sliding window (script Lua atomique)
- Retries exponentiels avec backoff + jitter
- Fallback m√©moire si Redis indisponible
- Log sanitizer pour masquer les donn√©es sensibles

### 4. SSE streaming fonctionnel
L'endpoint `/v1/ai/chat` supporte le streaming avec :
- Chunks `{"delta": "..."}` en temps r√©el
- √âv√©nement final `{"done": true, "text": "...", "usage": {...}}`
- Propagation des erreurs dans le stream

### 5. Migration transparente
Les services existants (ChatGuidanceService, GuidanceService) ont √©t√© migr√©s sans modification du contrat API REST :
- L'adaptateur encapsule la complexit√© du AI Engine
- Les endpoints `/v1/chat/*` et `/v1/guidance/*` restent identiques

### 6. Interpr√©tation du th√®me natal
Le parser `_parse_interpretation_sections()` extrait automatiquement :
- Synth√®se (section 1)
- Points cl√©s (section 2)
- Conseils (section 3)
- Disclaimer (section 4)

---

## Ce qui peut √™tre am√©lior√© ‚ö†Ô∏è

### 1. Nombre √©lev√© de rounds de code review
- Story 15.1 : 11 rounds
- Story 15.2 : 3 rounds
- Story 15.3 : 11 rounds
- Story 15.4 : 2 rounds

**Causes identifi√©es :**
- Oublis de tests (AC non couverts)
- Patterns non thread-safe corrig√©s it√©rativement
- Documentation incompl√®te des fichiers modifi√©s

**Action :** Checklist de v√©rification avant soumission incluant : tests pour chaque AC, thread-safety, File List compl√®te.

### 2. Complexit√© de l'adaptateur
L'AIEngineAdapter avec ses "test generators" est devenu complexe. Le pattern `set_test_chat_generator()` fonctionne mais n'est pas intuitif pour les nouveaux d√©veloppeurs.

**Action :** Documenter le pattern de test injection dans les conventions de test.

### 3. Cache d√©sactiv√© pour les interpr√©tations
`natal_chart_interpretation` utilise `temperature=0.7` > seuil de cache (0.0), donc le cache n'est jamais utilis√© pour ce use_case.

**Action :** √âvaluer si un cache avec TTL court (5 min) serait acceptable malgr√© la non-d√©terminisme, ou documenter explicitement pourquoi le cache est d√©sactiv√©.

### 4. Gestion async complexe
La conversion des handlers FastAPI de `def` vers `async def` a n√©cessit√© plusieurs it√©rations. Le pattern `asyncio.run()` dans les wrappers sync n'est pas optimal.

**Action :** Privil√©gier `async def` d√®s le d√©part pour tous les nouveaux handlers.

---

## Le√ßons apprises üìö

| Le√ßon | Recommandation |
|-------|----------------|
| Le script Lua Redis est atomique et √©limine les race conditions | Utiliser des scripts Lua pour les op√©rations multi-commandes Redis |
| Le singleton provider client doit √™tre thread-safe | Utiliser `threading.Lock` pour les initialisations paresseuses |
| Les erreurs SSE ne doivent pas √™tre re-raised apr√®s √©mission | Terminer le stream proprement, logger l'erreur |
| Le parsing de texte structur√© (sections num√©rot√©es) est fragile | Logger un warning si < 4 sections pars√©es, fallback gracieux |
| Les handlers async n√©cessitent `await` pour les services async | V√©rifier la cha√Æne compl√®te async lors de l'int√©gration |

---

## M√©triques

| M√©trique | Valeur |
|----------|--------|
| Stories compl√©t√©es | 4/4 (100%) |
| Tests ajout√©s | ~200 (54 AI Engine + 114 rate/cache/logs + 18 adapter + 36 interpretation) |
| Rounds de code review | 27 total (11 + 3 + 11 + 2) |
| Nouveaux fichiers | ~35 (module ai_engine complet + services + tests + Docker) |
| Lint errors final | 0 |

---

## Highlights techniques

### Rate Limiter Redis (script Lua atomique)
```lua
-- Sliding window rate limit
redis.call('ZREMRANGEBYSCORE', key, 0, window_start)
local count = redis.call('ZCARD', key)
if count >= limit then return {0, (window_start + window_size - now) * 1000}
redis.call('ZADD', key, now, now)
redis.call('EXPIRE', key, window_size + 1)
return {1, 0}
```

### SSE Streaming avec propagation d'erreurs
```python
async def _stream_chat(...):
    try:
        async for chunk in provider.chat_stream(...):
            yield {"delta": chunk.content}
    except Exception as e:
        yield {"error": {"type": type(e).__name__, "message": str(e)}}
        # Ne pas re-raise ‚Äî le stream est d√©j√† termin√©
```

### Parsing sections interpr√©tation
```python
def _parse_interpretation_sections(text: str) -> dict:
    sections = re.split(r'\n(?=\d+\.)', text.strip())
    return {
        "summary": sections[0] if len(sections) > 0 else "",
        "key_points": sections[1].split('\n- ') if len(sections) > 1 else [],
        "advice": sections[2].split('\n- ') if len(sections) > 2 else [],
        "disclaimer": sections[3] if len(sections) > 3 else "",
    }
```

---

## Actions pour les prochains sprints

1. [ ] **Documentation pattern test injection** ‚Äî Documenter `set_test_*_generator()` dans les conventions de test
2. [ ] **√âvaluation cache temperature > 0** ‚Äî D√©cider si un TTL court est acceptable pour les interpr√©tations
3. [ ] **Checklist pre-submit** ‚Äî Cr√©er une checklist incluant : AC ‚Üí tests, thread-safety, File List, async chain
4. [ ] **Suppression compl√®te LLMClient** ‚Äî L'ancien stub est marqu√© d√©pr√©ci√©, planifier sa suppression d√©finitive
5. [ ] **Monitoring Prometheus** ‚Äî Connecter les m√©triques `ai_engine_*` √† un dashboard Grafana

---

## Dettes techniques identifi√©es

| Dette | Priorit√© | Description |
|-------|----------|-------------|
| Cache d√©sactiv√© pour interpr√©tations | LOW | Temperature 0.7 > seuil, cache jamais utilis√© |
| LLMClient encore pr√©sent (d√©pr√©ci√©) | LOW | √Ä supprimer d√©finitivement |
| Pattern asyncio.run() dans wrappers sync | MEDIUM | √Ä remplacer par async natif |
| Test e2e AI Engine | MEDIUM | Pas de test e2e complet avec vrai appel OpenAI |

---

**Status :** Epic 15 ‚úÖ Done
